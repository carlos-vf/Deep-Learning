# Tracker Performance Evaluation (`evaluate_f4k.py`)

## Overview

The script (`evaluate_f4k.py`) is a dedicated tool for quantitatively evaluating the performance of the fish tracking pipeline over the **Fish4Knowledge** data videos. It compares the output generated by your tracker against a pre-annotated ground truth dataset to calculate standard industry metrics for Multi-Object Tracking (MOT).

This allows you to get objective, numerical scores for your tracker's accuracy, helping you tune parameters and measure improvements.


---

## How to Execute
You must run this script from the root directory of your project. It requires two command-line arguments:

- `--gt-xml`: The path to the ground truth .xml file for the video you want to evaluate.

- `--ts-txt`: The path to the corresponding tracker results `.txt` file generated by your pipeline.

Example Command:

```bash
py src/tracker/evaluate_f4k.py --gt-xml "data/f4k/gt_113.xml" --ts-txt "outputs/gt_113_realtime_results.txt"
```

---

## Understanding the Ouput

Some of the key metrics are:

- **MOTA (Multi-Object Tracking Accuracy)**: The main "overall" score for tracking quality. It combines false positives, false negatives, and ID switches. Higher is better.

- **IDF1 (ID F1 Score)**: The best metric for measuring ID consistency. It balances how precisely you assign IDs (IDP) and how completely you track objects (IDR). Higher is better.

- **Rcll (Recall)**: What percentage of the real fish did your detector find? A low recall is often the biggest reason for a low MOTA score. Higher is better.

- **Prcn (Precision)**: Of the fish your system detected, what percentage were actually correct? Higher is better.

- **IDs (ID Switches)**: The total number of times a tracked object's ID was incorrectly changed. Lower is better.

- **FN (False Negatives)**: The total number of fish that were missed by your detector. Lower is better.

- **FP (False Positives)**: The total number of incorrect detections (e.g., detecting a rock as a fish). Lower is better.
